{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e807d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import spacy\n",
    "import GPUtil\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import altair as alt\n",
    "from altair import Chart\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df665c",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The positional encoding module is added, for the transformer to understand relative word positions, this is, absolute positions within the text but also in relation to each other. Periodical functions (sine and cosine) are used, as their orthogonality allows for unique encodings to be described through combinations of them (trigonometric identities). In addition, a dropout layer is added after the PE to avoid overfitting during training, as it prevents over-dependence on exact token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d718bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000)/d_model))  #Exp for (math) convenience\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  #Add batch dimension for input\n",
    "        self.register_buffer(\"pe\", pe)   #Register positional encoding as non-updatable tensor (not parameter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].detach()  #Adjust to input size, stop gradient flowing through PE\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794122",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "The Multi-Head Attention module uses several sets of Query (Q), Key (K) and Value (V) matrices, where each set of matrices(belonging to a head) will capture information about the text in a different regard. This is called an Attention Module.\n",
    "\n",
    "For instance, Head 1 with matrices {K_1, Q_1, V_1} will extract the semantic information, while Head 2 with matrices {K_2, Q_2, V_2} will extract syntactic information. Each of these modules will compute a weighted sum of the attention probabilities.\n",
    "\n",
    "In the end, the weighted sums are concatenated and projected through a final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01172ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == d_model      #Ensure integer dimensions\n",
    "\n",
    "        #Linear (affine) transformations. Single-layer NNs acting as matrices\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        K_t = K.transpose(-2, -1)                     #Transpose to match dimensions and get right similarity scores\n",
    "        attn_scores = torch.matmul(Q, K_t) / scale\n",
    "\n",
    "        #We want to remove the similarity scores of zero from attn_scores, but softmax will turn them to 1 because exp(0)=1. \n",
    "        #The mask transforms those logits so exp(-1e9)=0 and they don't receive attention after softmax is applied\n",
    "        if mask != None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        return torch.matmul(attn_probs, V)\n",
    "\n",
    "    def _project(self, x, linear):\n",
    "        #Project and reshape, as output of projection has shape [batch_size, sequence_length, d_model]\n",
    "        batch_size = x.size(0)\n",
    "        return linear(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, mask=None): \n",
    "        #Extract batch size (0th dimension)\n",
    "        batch_size = x_q.size(0)\n",
    "\n",
    "        #Project input to learnable spaces\n",
    "        Q_proj = self._project(x_q, self.W_q)       \n",
    "        K_proj = self._project(x_k, self.W_k)\n",
    "        V_proj = self._project(x_v, self.W_v)\n",
    "\n",
    "        #Attention for each head\n",
    "        attn_output = self.scaled_dot_product_attention(Q_proj, K_proj, V_proj, mask)\n",
    "\n",
    "        #Concatenate heads and reshape vector to size=d_model\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        #Project concatenated heads\n",
    "        return self.W_o(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fae84b",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "The Feed Forward network is placed after the Multi-Head Attention layer. \n",
    "\n",
    "It is used to individually process each token in the sequence, independently of the other tokens, and it simply consists of a two-layer neural network, with an activation function (ReLU in this case) in the middle to introduce non-linearity. \n",
    "\n",
    "Dropout is added after the activation function to again avoid overfitting. The dimensionality of the inner layer must be larger than *d_model*, but the exact ratio can vary from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a695dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_layer1 = self.dropout(F.relu(self.linear1(x)))    # Input goes through NN, mapped through ReLU and used dropout\n",
    "        return self.linear2(x_layer1)                       # Second NN, restored to original dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ebb5d",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "The Encoder Layer (which is repeated n times, one after the other) extracts the input sequence's most relevant features. The input is processed through the Multi-Head Self Attention module, whose output (with dropout) is added to the input to form a residual connection layer for a smooth gradient flow without it disappearing, and then normalized through Layer Normalization (common for NLP). After normalization, the output goes through the Feed Forward network, and forms a residual connection which is again normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65a8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x)            # Multi-Head Attention (all same x here)\n",
    "        res1 = x + self.dropout(attn_output)             # Residual connection\n",
    "        norm_layer1 = self.norm1(res1)                   # Layer Normalization\n",
    "        ffn_output = self.ffn(norm_layer1)               # Feed Forward\n",
    "        res2 = norm_layer1 + self.dropout(ffn_output)    # Residual connection\n",
    "        return self.norm2(res2)                          # Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ad26a",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "The Decoder Layer, which processes the model's output, is similar to the Encoder layer, as it also consist of n consecutive repetitions of itself, a Multi-Head Self Attention module and a Feed Forward network.\n",
    "\n",
    "The main difference lies in that, between the Self Attention and the Feed Forward network, there is a Cross Attention Module, which takes the output from the Encoder Layer as its Key and Value inputs, and the output from the (normalized) Self Attention as the Query input. This is done so the decoder can also process the relevant features about the original input sequence, and link it to the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fdb51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask, cross_mask):\n",
    "        attn_output = self.self_attn(x, x, x, self_mask)                                                # Multi-Head Attention\n",
    "        res1 = x + self.dropout(attn_output)                                                            # Residual connection\n",
    "        norm_layer1 = self.norm1(res1)                                                                  # Layer Normalization\n",
    "\n",
    "        cross_attn_output = self.cross_attn(norm_layer1, encoder_output, encoder_output, cross_mask)    # Cross Attention\n",
    "        res2 = norm_layer1 + self.dropout(cross_attn_output)                                            # Residual connection\n",
    "        norm_layer2 = self.norm2(res2)                                                                  # Layer Normalization\n",
    "\n",
    "        ffn_output = self.ffn(norm_layer2)                                                              # Feed Forward\n",
    "        res3 = norm_layer2 + self.dropout(ffn_output)                                                   # Residual Connection\n",
    "        return self.norm3(res3)                                                                         # Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78e193",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f029ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model=512, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(source_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0.1)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, inputs, outputs, enc_mask=None, dec_mask=None):\n",
    "        inputs_embedding = self.positional_encoding(self.encoder_embedding(inputs))     # Embed input and add positional encoding\n",
    "        encoder_input = inputs_embedding\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_input, enc_mask)                             # Perform n layers of encoder\n",
    "\n",
    "        outputs_embedding = self.positional_encoding(self.decoder_embedding(outputs))   # Embed output and add positional encoding\n",
    "        decoder_input = outputs_embedding\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_input, encoder_output, enc_mask, dec_mask)   #Perform n layers of decoder\n",
    "\n",
    "        return self.fc_out(decoder_output)                                              # Affine transform to reduce dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa837e",
   "metadata": {},
   "source": [
    "## Transformer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a23d18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. Positional Encoding Test\n",
      "Before PE: mean=-0.0059, std=0.9872\n",
      "After PE: mean=0.4742, std=0.9872\n",
      "PE shape: torch.Size([1, 10, 512]). (should be [1, 10, 512])\n",
      "\n",
      "2. Multi-Head Attention Test\n",
      "Attention output shape: torch.Size([2, 10, 512]). (should be torch.Size([2, 10, 512]))\n",
      "Max value: 0.3594\n",
      "Min value: -0.3447\n",
      "\n",
      "3. Encoder test\n",
      "Encoder output shape: torch.Size([2, 10, 512]). (should be torch.Size([2, 10, 512]))\n",
      "Data changed: False. (should be False)\n",
      "\n",
      "4. Decoder test\n",
      "Decoder output shape: torch.Size([2, 10, 512]). (should be torch.Size([2, 10, 512]))\n",
      "Output norm = 101.1924\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "    torch.manual_seed(24)\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    source_vocab_size = 100\n",
    "    target_vocab_size = 100\n",
    "    num_layers = 2\n",
    "\n",
    "    source = torch.randint(0, source_vocab_size, (batch_size, seq_len))\n",
    "    target = torch.randint(0, target_vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    enc_mask = torch.ones(batch_size, 1, 1, seq_len)\n",
    "    dec_mask = torch.tril(torch.ones(seq_len, seq_len)).expand(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    transformer = Transformer(source_vocab_size=source_vocab_size,\n",
    "                              target_vocab_size=target_vocab_size,\n",
    "                              d_model=d_model,\n",
    "                              num_heads=num_heads,\n",
    "                              num_layers=num_layers)\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Positional Encoding Test\")\n",
    "    pe = PositionalEncoding(d_model, dropout=0.1)\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    print(f\"Before PE: mean={x.mean():.4f}, std={x.std():.4f}\")\n",
    "    x_pe = pe(x)\n",
    "    print(f\"After PE: mean={x_pe.mean():.4f}, std={x.std():.4f}\")\n",
    "    print(f\"PE shape: {x_pe.shape}. (should be [1, {seq_len}, {d_model}])\")\n",
    "\n",
    "    print(\"\\n2. Multi-Head Attention Test\")\n",
    "    att = MultiHeadAttention(d_model, num_heads)\n",
    "    q = k = v = torch.randn(batch_size, seq_len, d_model)\n",
    "    att_output = att(q,k,v)\n",
    "    print(f\"Attention output shape: {att_output.shape}. (should be {q.shape})\")\n",
    "    print(f\"Max value: {att_output.max():.4f}\")\n",
    "    print(f\"Min value: {att_output.min():.4f}\")\n",
    "\n",
    "    print(\"\\n3. Encoder test\")\n",
    "    encoder_layer = EncoderLayer(d_model, num_heads)\n",
    "    encoder_input = torch.randn(batch_size, seq_len, d_model)\n",
    "    encoder_output = encoder_layer(encoder_input)\n",
    "    print(f\"Encoder output shape: {encoder_output.shape}. (should be {encoder_input.shape})\")\n",
    "    print(f\"Data changed: {torch.allclose(encoder_input, encoder_output, atol=1e-4)}. (should be False)\")\n",
    "\n",
    "    print(\"\\n4. Decoder test\")\n",
    "    decoder_layer = DecoderLayer(d_model, num_heads)\n",
    "    decoder_input = torch.randn(batch_size, seq_len, d_model)\n",
    "    decoder_output = decoder_layer(decoder_input, encoder_output, enc_mask, dec_mask)\n",
    "    print(f\"Decoder output shape: {decoder_output.shape}. (should be {decoder_input.shape})\")\n",
    "    print(f\"Output norm = {decoder_output.norm():.4f}\")\n",
    "\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083310b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
