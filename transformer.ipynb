{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import spacy\n",
    "import GPUtil\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import altair as alt\n",
    "from altair import Chart\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df665c",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The positional encoding module is added, for the transformer to understand relative word positions, this is, absolute positions within the text but also in relation to each other. Periodical functions (sine and cosine) are used, as their orthogonality allows for unique encodings to be described through combinations of them (trigonometric identities). In addition, a dropout layer is added after the PE to avoid overfitting during training, as it prevents over-dependence on exact token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)).float() * (-math.log(10000)/d_model)  #Exp for (math) convenience\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  #Add batch dimension for input\n",
    "        self.register_buffer(\"pe\", pe)   #Register positional encoding as non-updatable tensor (not parameter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].detach()  #Adjust to input size, stop gradient flowing through PE\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794122",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "The Multi-Head Attention module uses several sets of Query (Q), Key (K) and Value (V) matrices, where each set of matrices(belonging to a head) will capture information about the text in a different regard. This is called an Attention Module.\n",
    "\n",
    "For instance, Head 1 with matrices {K_1, Q_1, V_1} will extract the semantic information, while Head 2 with matrices {K_2, Q_2, V_2} will extract syntactic information. Each of these modules will compute a weighted sum of the attention probabilities.\n",
    "\n",
    "In the end, the weighted sums are concatenated and projected through a final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01172ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        assert head_dim * num_heads == d_model      #Ensure integer dimensions\n",
    "\n",
    "        #Linear (affine) transformations, acting like matrices\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=False):\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        K_t = K.transpose(-2, -1)    #Transpose to match dimensions and get right similarity scores\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K_t) / scale\n",
    "\n",
    "        #We want to remove the similarity scores of zero from attn_scores, but softmax will turn them to 1 because exp(0)=1. \n",
    "        #The mask transforms those logits so exp(-1e9)=0 and they don't receive attention after softmax is applied\n",
    "        if mask == True:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        return torch.matmul(attn_probs, V)\n",
    "\n",
    "    def _project(self, x, linear):\n",
    "        #Project and reshape, as output of projection has shape [batch_size, sequence_length, d_model]\n",
    "        batch_size = x.size(0)\n",
    "        return linear(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, K, Q, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        Q_proj = self._project(Q, self.W_q)\n",
    "        K_proj = self._project(K, self.W_k)\n",
    "        V_proj = self._project(V, self.W_v)\n",
    "\n",
    "        #Attention for each head\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        #Concatenate heads and reshape vector to size=d_model\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        #Project concatenated heads\n",
    "        return self.W_o(attn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
